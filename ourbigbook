#!/usr/bin/env node

const { performance } = require('perf_hooks');

const china_dictatorship = require('china-dictatorship');
if (!china_dictatorship.get_data().includes("Tiannmen Square protests")) throw 0;

const child_process = require('child_process');
const fs = require('fs');
const path = require('path');

// This library is terrible, too much magic, hard to understand interface,
// does not do some obvious basics.
const commander = require('commander');
const is_installed_globally = require('is-installed-globally');
const readCb = require('read');
const { Liquid } = require('liquidjs');
const { DataTypes, Op, Sequelize } = require('sequelize')

const ourbigbook = require('ourbigbook');
const ourbigbook_nodejs = require('ourbigbook/nodejs');
const ourbigbook_nodejs_webpack_safe = require('ourbigbook/nodejs_webpack_safe');
const { WebApi, read_include } = require('ourbigbook/web_api')

const OURBIGBOOK_TEX_BASENAME = 'ourbigbook.tex';
const LOG_OPTIONS = new Set([
  'ast',
  'ast-simple',
  'db',
  'headers',
  'tokens',
]);
const SASS_EXT = '.scss';
// TODO get rid of this/most of this, and instead use the actual gitignore.
// https://github.com/cirosantilli/ourbigbook/issues/253
const DEFAULT_IGNORE_BASENAMES = [
  '.git',
  '.gitignore',
  '.sass-cache',
  '.venv',
  'main.liquid.html',
  'node_modules',
  'package-lock.json',
  'package.json',
  ourbigbook_nodejs_webpack_safe.TMP_DIRNAME,
  ourbigbook_nodejs.PACKAGE_SASS_BASENAME,
];
const DEFAULT_IGNORE_BASENAMES_SET = new Set(DEFAULT_IGNORE_BASENAMES);
const SQLITE_MAGIC_MEMORY_NAME = ':memory:'
exports.SQLITE_MAGIC_MEMORY_NAME = SQLITE_MAGIC_MEMORY_NAME

class DbProviderDbAdapter {
  constructor(non_ourbigbook_options) {
  }
}

async function read(opts) {
  return new Promise((resolve, reject) => {
    readCb(opts, (err, line) => {
      resolve([err, line])
    })
  })
}

// Reconcile the database with information that depends only on existence of Ourbigbook files, notably:
// - remove any IDs from deleted files https://github.com/cirosantilli/ourbigbook/issues/125
// - prefetch all last_render timestamps in one go
async function reconcile_db_and_filesystem(input_path, ourbigbook_options, non_ourbigbook_options) {
  const sequelize = non_ourbigbook_options.sequelize
  if (sequelize) {
    const new_non_ourbigbook_options = ourbigbook.clone_and_set(
      non_ourbigbook_options, 'ourbigbook_paths_converted_only', true)
    await convert_directory(
      input_path,
      ourbigbook_options,
      new_non_ourbigbook_options,
    );
    const ourbigbook_paths_converted = new_non_ourbigbook_options.ourbigbook_paths_converted
    const [,,file_rows] = await Promise.all([
      sequelize.models.Id.destroy({
        where: { path: { [Op.not]: ourbigbook_paths_converted } } }),
      sequelize.models.Ref.destroy({
        where: { defined_at: { [Op.not]: ourbigbook_paths_converted } } }),
      sequelize.models.File.findAll({
        where: { path: ourbigbook_paths_converted } }),
    ])
    const file_rows_dict = {}
    for (const file_row of file_rows) {
      file_rows_dict[file_row.path] = file_row
    }
    non_ourbigbook_options.file_rows_dict = file_rows_dict
  }
}

// Do various post conversion checks to verify database integrity:
//
// - duplicate IDs
// - https://docs.ourbigbook.com/x-within-title-restrictions
//
// Previously these were done inside ourbigbook.convert. But then we started skipping render by timestamp,
// so if you e.g. move an ID from one file to another, a common operation, then it would still see
// the ID in the previous file depending on conversion order. So we are moving it here instead at the end.
// Having this single query at the end also be slightly more efficient than doing each query separately per file converion.
async function check_db(non_ourbigbook_options) {
  const sequelize = non_ourbigbook_options.sequelize
  if (sequelize && non_ourbigbook_options.commander.render) {
    const error_messages = await ourbigbook_nodejs_webpack_safe.check_db(
      sequelize, non_ourbigbook_options.ourbigbook_paths_converted)
    if (error_messages.length > 0) {
      cli_error('\n' + error_messages.join('\n'))
    }
  }
}

/** Report an error with the CLI usage and exit in error. */
function cli_error(message) {
  console.error(`error: ${message}`);
  process.exit(1);
}

function cmd_get_stdout(cmd, args=[], options={}) {
  if (!('dry_run' in options)) {
    options.dry_run = false;
  }
  if (!('env_extra' in options)) {
    options.env_extra = {};
  }
  if (!('show_cmd' in options)) {
    options.show_cmd = true;
  }
  let out;
  const cmd_str = ([cmd].concat(args)).join(' ');
  if (options.show_cmd) {
    console.log(cmd_str);
  }
  if (!options.dry_run) {
    out = child_process.spawnSync(cmd, args, { env: { ...process.env, ...options.env_extra } });
  }
  let ret;
  if (options.dry_run) {
    ret = '';
  } else {
    if (out.status != 0 && options.throw_on_error) {
      let msg = `cmd: \n${cmd_str}\n`;
      if (out.stdout !== null) {
        msg += `stdout: \n${out.stdout.toString(ourbigbook_nodejs_webpack_safe.ENCODING)}\n`;
      }
      if (out.stderr !== null) {
        msg += `stderr: \n${out.stderr.toString(ourbigbook_nodejs_webpack_safe.ENCODING)}\n`;
      }
      throw msg;
    }
    ret = out.stdout.toString(ourbigbook_nodejs_webpack_safe.ENCODING);
  }
  return ret;
}

/**
 * @param {String} input_path - path to a directory to convert files in
 */
async function convert_directory(input_path, ourbigbook_options, non_ourbigbook_options) {
  non_ourbigbook_options.ourbigbook_paths_converted = []
  const ignore_fullpaths = new Set();
  for (const ignore of ourbigbook_options.ourbigbook_json.ignore) {
    ignore_fullpaths.add(path.join(non_ourbigbook_options.ourbigbook_json_dir, ignore));
  }
  for (const one_path of walk_files_recursively(
    input_path,
    DEFAULT_IGNORE_BASENAMES_SET,
    ignore_fullpaths,
  )) {
    await convert_path_to_file(
      one_path,
      ourbigbook_options,
      non_ourbigbook_options,
    );
    if (non_ourbigbook_options.had_error) {
      break;
    }
  }
}

/** Extract IDs from all input files into the ID database, without fully converting. */
async function convert_directory_extract_ids(input_path, ourbigbook_options, non_ourbigbook_options) {
  await convert_directory(
    input_path,
    ourbigbook.clone_and_set(ourbigbook_options, 'render', false),
    non_ourbigbook_options
  );
}

async function convert_directory_extract_ids_and_render(input_dir, ourbigbook_options, non_ourbigbook_options) {
  await reconcile_db_and_filesystem(input_dir, ourbigbook_options, non_ourbigbook_options);
  await convert_directory_extract_ids(input_dir, ourbigbook_options, non_ourbigbook_options);
  if (!non_ourbigbook_options.had_error) {
    await check_db(non_ourbigbook_options)
  }
  if (non_ourbigbook_options.commander.render) {
    if (!non_ourbigbook_options.had_error) {
      await convert_directory(
        input_dir,
        ourbigbook_options,
        ourbigbook.clone_and_set(non_ourbigbook_options, 'is_render_after_extract', true)
      );
    }
  }
}

/** Convert input from a string to output and return the output as a string.
 *
 * Wraps ourbigbook.convert with CLI usage convenience.
 *
 * @param {String} input
 * @param {Object} options - options to be passed to ourbigbook.convert
 * @param {Object} non_ourbigbook_options - control options for this function,
 *                 not passed to ourbigbook.convert. Also contains some returns:
 *                 - {bool} had_error
 *                 - {Object} extra_returns
 * @return {String}
 */
async function convert_input(input, ourbigbook_options, non_ourbigbook_options={}) {
  const new_options = Object.assign({}, ourbigbook_options);
  if ('input_path' in non_ourbigbook_options) {
    new_options.input_path = non_ourbigbook_options.input_path;
  }
  if ('title' in non_ourbigbook_options) {
    new_options.title = non_ourbigbook_options.title;
  }
  new_options.extra_returns = {};
  // If we don't where the output will go (the case for stdout) or
  // the user did not explicitly request full embedding, inline all CSS.
  // Otherwise, include and external CSS to make each page lighter.
  if (non_ourbigbook_options.commander.embedResources) {
    new_options.template_vars.style = fs.readFileSync(
      ourbigbook_nodejs.DIST_CSS_PATH,
      ourbigbook_nodejs_webpack_safe.ENCODING
    );
    new_options.template_vars.post_body = `<script>${fs.readFileSync(
      ourbigbook_nodejs.DIST_JS_PATH, ourbigbook_nodejs_webpack_safe.ENCODING)}</script>\n`;
     ;
  } else {
    let includes_str = ``;
    let scripts_str = ``;
    let includes = [];
    let scripts = [];
    let includes_local = [];
    let scripts_local = [];
    let template_includes_relative = [];
    let template_scripts_relative = [];
    if (non_ourbigbook_options.publish) {
      template_includes_relative.push(
        path.relative(
          non_ourbigbook_options.outdir,
          non_ourbigbook_options.out_css_path
        )
      );
      template_scripts_relative.push(
        path.relative(
          non_ourbigbook_options.outdir,
          non_ourbigbook_options.out_js_path
        )
      );
    } else {
      includes_local.push(non_ourbigbook_options.out_css_path);
      scripts_local.push(non_ourbigbook_options.out_js_path);
    }
    if (
      ourbigbook_options.outfile !== undefined &&
      !is_installed_globally
    ) {
      for (const include of includes_local) {
        includes.push(path.relative(path.dirname(ourbigbook_options.outfile), include));
      }
      for (const script of scripts_local) {
        scripts.push(path.relative(path.dirname(ourbigbook_options.outfile), script));
      }
    } else {
      includes.push(...includes_local);
      scripts.push(...scripts_local);
    }

    for (const include of includes) {
      includes_str += `@import "${include}";\n`;
    }
    for (const script of scripts) {
      scripts_str += `<script src="${script}"></script>\n`;
    }
    new_options.template_vars.style = `\n${includes_str}`;
    new_options.template_vars.post_body = `${scripts_str}`;
    new_options.template_styles_relative = template_includes_relative;
    new_options.template_scripts_relative = template_scripts_relative;
  }
  // Finally, do the conversion!
  const output = await ourbigbook.convert(input, new_options, new_options.extra_returns);
  if (non_ourbigbook_options.post_convert_callback) {
    await non_ourbigbook_options.post_convert_callback(non_ourbigbook_options.input_path, new_options.extra_returns)
  }
  if (non_ourbigbook_options.log.tokens) {
    console.error('tokens:');
    console.error(JSON.stringify(new_options.extra_returns.tokens, null, 2));
    console.error();
  }
  if (non_ourbigbook_options.log.ast) {
    console.error('ast:');
    console.error(JSON.stringify(new_options.extra_returns.ast, null, 2));
    console.error();
  }
  if (non_ourbigbook_options.log['ast-simple']) {
    console.error('ast-simple:');
    console.error(new_options.extra_returns.ast.toString());
    console.error();
  }
  // Remove duplicate messages due to split header rendering. We could not collect
  // errors from that case at all maybe, but do we really want to run the risk of
  // missing errors?
  for (const error_string of ourbigbook_nodejs_webpack_safe.remove_duplicates_sorted_array(
      new_options.extra_returns.errors.map(e => e.toString()))) {
    console.error(error_string);
  }
  non_ourbigbook_options.extra_returns = new_options.extra_returns;
  if (new_options.extra_returns.errors.length > 0) {
    non_ourbigbook_options.had_error = true;
  }
  ourbigbook.perf_print(new_options.extra_returns.context, 'convert_input_end')
  return output;
}

/** Convert filetypes that ourbigbook knows how to convert, and just copy those that we don't, e.g.:
 *
 * * .bigb to .html
 * * .scss to .css
 *
 * @param {string} input_path - path relative to the base_path, e.g. `./ourbigbook subdir` gives:
 *   base_path: "subdir" and input_path "index.bigb" amongst other files.
 *
 * The output file name is derived from the input file name with the output extension.
 */
async function convert_path_to_file(input_path, ourbigbook_options, non_ourbigbook_options={}) {
  let input_path_parse = path.parse(input_path);
  let msg_ret
  let output, first_output_path;
  let timestamp_skip = false
  if (input_path_parse.ext === `.${ourbigbook.OURBIGBOOK_EXT}`) {
    let full_path = path.resolve(input_path);
    let input_path_parse = path.parse(full_path);
    let path_relative_to_ourbigbook_json;
    let input_path_relative_to_ourbigbook_json;
    if (non_ourbigbook_options.ourbigbook_json_dir !== undefined) {
      path_relative_to_ourbigbook_json = path.relative(non_ourbigbook_options.ourbigbook_json_dir, input_path_parse.dir);
      input_path_relative_to_ourbigbook_json = path.join(path_relative_to_ourbigbook_json, input_path_parse.base);
    }
    non_ourbigbook_options.ourbigbook_paths_converted.push(input_path_relative_to_ourbigbook_json)
    if (non_ourbigbook_options.ourbigbook_paths_converted_only) {
      return
    }
    msg_ret = convert_path_to_file_print_starting(ourbigbook_options, input_path)
    let new_options = Object.assign({}, ourbigbook_options);
    let new_non_ourbigbook_options = Object.assign({}, non_ourbigbook_options);
    let input = fs.readFileSync(full_path, new_non_ourbigbook_options.encoding);
    let input_path_basename_noext = input_path_parse.name;
    const sequelize = non_ourbigbook_options.sequelize
    if (input_path_relative_to_ourbigbook_json) {
      const file_row = non_ourbigbook_options.file_rows_dict[input_path_relative_to_ourbigbook_json]
      if (file_row !== undefined) {
        const mtime = fs.statSync(input_path).mtime
        if (
          ourbigbook_options.render
        ) {
          timestamp_skip = non_ourbigbook_options.commander.renderTimestamp &&
            file_row.last_render !== null &&
            file_row.last_render > mtime &&
            // Some day we might have one timestamp per output format.
            // But lazy now, just never use timestamp for --format-source.
            !non_ourbigbook_options.commander.formatSource
        } else {
          timestamp_skip = file_row.last_parse !== null && file_row.last_parse > mtime
        }
      }
    }
    if (!timestamp_skip) {
      new_non_ourbigbook_options.input_path = input_path_relative_to_ourbigbook_json;
      if (sequelize) {
        new_options.db_provider = non_ourbigbook_options.db_provider;
      }

      // Convert.
      output = await convert_input(input, new_options, new_non_ourbigbook_options);
      const extra_returns = new_non_ourbigbook_options.extra_returns
      if (
        non_ourbigbook_options.commander.formatSource &&
        ourbigbook_options.render
      ) {
        if (!new_non_ourbigbook_options.had_error) {
          fs.writeFileSync(full_path, output);
        }
        first_output_path = full_path
      } else {
        // Write out the output the output files.
        for (const outpath in extra_returns.rendered_outputs) {
          const output_path = path.join(non_ourbigbook_options.outdir, outpath);
          if (output_path === full_path) {
            cli_error(`output path equals input path: "${outpath}"`);
          }
          if (first_output_path === undefined) {
            first_output_path = output_path
          }
          fs.mkdirSync(path.dirname(output_path), { recursive: true });
          fs.writeFileSync(output_path, extra_returns.rendered_outputs[outpath].full);
        }
      }
      if (new_options.split_headers) {
        for (const header_ast of extra_returns.context.synonym_headers) {
          let new_options_redir = Object.assign({}, new_options);
          new_options_redir.db_provider = extra_returns.context.db_provider;
          await generate_redirect(new_options_redir, header_ast.id, header_ast.synonym, non_ourbigbook_options.outdir);
        }
      }

      const context = extra_returns.context;
      if (non_ourbigbook_options.log.headers) {
        console.error(context.header_tree.toString());
      }

      // Update the Sqlite databse with results from the conversion.
      ourbigbook.perf_print(context, 'convert_path_pre_sqlite')
      if ('sequelize' in non_ourbigbook_options) {
        await ourbigbook_nodejs_webpack_safe.update_database_after_convert({
          extra_returns,
          db_provider: non_ourbigbook_options.db_provider,
          non_ourbigbook_options,
          sequelize: non_ourbigbook_options.sequelize,
          path: input_path_relative_to_ourbigbook_json,
          render: ourbigbook_options.render,
          is_render_after_extract: non_ourbigbook_options.is_render_after_extract,
        })
      }
      if (new_non_ourbigbook_options.had_error) {
        non_ourbigbook_options.had_error = true;
      }
    }
  } else {
    if (
      non_ourbigbook_options.commander.formatSource ||
      non_ourbigbook_options.ourbigbook_paths_converted_only
    ) {
      // I should use callbacks instead of doing this. But lazy.
      return
    }
    let output_name = input_path_parse.name;
    let output_path_noext = path.join(
      path.relative(non_ourbigbook_options.ourbigbook_json_dir, input_path_parse.dir),
      output_name
    );
    if (ourbigbook_options.outfile === undefined) {
      output_path_noext = path.join(non_ourbigbook_options.outdir, output_path_noext);
    } else {
      output_path_noext = ourbigbook_options.outfile;
    }
    fs.mkdirSync(path.dirname(output_path_noext), {recursive: true});
    if (ourbigbook_options.render) {
      if (input_path_parse.ext === SASS_EXT) {
        first_output_path = output_path_noext + '.css'
        msg_ret = convert_path_to_file_print_starting(ourbigbook_options, input_path)
        fs.writeFileSync(
          first_output_path,
          require('sass').renderSync({
            data: fs.readFileSync(path.join(input_path), non_ourbigbook_options.encoding),
            outputStyle: 'compressed',
            includePaths: [
              path.dirname(ourbigbook_nodejs.PACKAGE_PATH),
            ],
          }).css
        );
      } else {
        // Otherwise, just copy the file over if needed.
        const output_path = output_path_noext + input_path_parse.ext;
        if (output_path !== path.resolve(input_path)) {
          console.log(`copy ${input_path} -> ${path.relative(process.cwd(), output_path)}`);
          fs.copyFileSync(input_path, output_path);
        }
      }
    }
  }

  if (msg_ret !== undefined) {
    let t1 = performance.now();
    let output_path_str
    if (
      ourbigbook_options.render &&
      // Happens if:
      // - conversion to .tex
      first_output_path !== undefined
    ) {
      output_path_str = ` -> ${path.relative(process.cwd(), first_output_path)}`
    } else {
      output_path_str = ''
    }
    let done_str
    if (timestamp_skip) {
      done_str = `skipped by timestamp`
    } else {
      done_str = `finished in ${t1 - msg_ret.t0} ms`
    }
    console.log(`${msg_ret.message}${output_path_str} ${done_str}`);
  }
  if (ourbigbook_options.perf) {
    console.error(`perf convert_path_to_file_end ${performance.now()}`);
  }
  return output;
}

const MESSAGE_PREFIX_EXTRACT_IDS = 'extract_ids'
const MESSAGE_PREFIX_RENDER = 'render'

function convert_path_to_file_print_starting(ourbigbook_options, input_path) {
  let message_prefix;
  if (ourbigbook_options.render) {
    message_prefix = MESSAGE_PREFIX_RENDER;
  } else {
    message_prefix = MESSAGE_PREFIX_EXTRACT_IDS;
  }
  const message = `${message_prefix} ${input_path}`;
  const t0 = performance.now()
  console.log(message);
  return { message, t0 };
}

async function create_db(ourbigbook_options, non_ourbigbook_options) {
  perf_print('create_db_begin', ourbigbook_options)
  const db_dir = path.dirname(non_ourbigbook_options.db_options.storage);
  if (!fs.existsSync(db_dir)) {
    fs.mkdirSync(db_dir, { recursive: true });
  }
  const sequelize = await ourbigbook_nodejs_webpack_safe.create_sequelize(
    non_ourbigbook_options.db_options,
    Sequelize,
    { force: commander.clearDb },
  )
  non_ourbigbook_options.sequelize = sequelize;
  non_ourbigbook_options.db_provider = new ourbigbook_nodejs_webpack_safe.SqliteDbProvider(sequelize);
  perf_print('create_db_end', ourbigbook_options)
}

async function generate_redirect(ourbigbook_options, redirect_src_id, redirect_target_id, outdir) {
  ourbigbook_options = Object.assign({}, ourbigbook_options);
  ourbigbook_options.input_path = redirect_src_id;
  const outpath_basename = redirect_src_id + '.' + ourbigbook.HTML_EXT
  const outpath = path.join(outdir, outpath_basename);
  ourbigbook_options.outfile = outpath_basename;
  const redirect_href = await ourbigbook.convert_x_href(redirect_target_id, ourbigbook_options);
  if (redirect_href === undefined) {
    cli_error(`redirection target ID "${redirect_target_id}" not found`);
  }
  generate_redirect_base(outpath, redirect_href)
}

function generate_redirect_base(outpath, redirect_href) {
  fs.mkdirSync(path.dirname(outpath), {recursive: true})
  // https://stackoverflow.com/questions/10178304/what-is-the-best-approach-for-redirection-of-old-pages-in-jekyll-and-github-page/36848440#36848440
  fs.writeFileSync(outpath,
`<!DOCTYPE html>
<html>
<head>
<meta charset="utf-8">
<title>Redirecting...</title>
<link rel="canonical" href="${redirect_href}"/>
<meta http-equiv="refresh" content="0;url=${redirect_href}" />
</head>
<body>
<h1>Redirecting...</h1>
<a href="${redirect_href}">Click here if you are not redirected.</a>
<script>location='${redirect_href}'</script>
</body>
</html>
`);
}

/** Return Set of branches in the repository. Hax. */
function git_branches(input_path) {
  const str = cmd_get_stdout('git', ['branch', '-a']).replace(/\n$/, '')
  const arr = (str === '') ? [] : str.split('\n');
  return new Set(arr.map(s => s.substring(2)));
}

function git_has_commit(input_path) {
  try {
    cmd_get_stdout('git', ['-C', input_path, 'log'], {throw_on_error: true})
    return true
  } catch(err) {
    return false
  }
}

function git_is_in_repo(input_path) {
  try {
    cmd_get_stdout('git', ['-C', input_path, 'status'], {throw_on_error: true})
    return true
  } catch(err) {
    return false
  }
}

/**
 * @return {String} full Git SHA of the source.
 */
function git_ls_files(input_path) {
  const ret = cmd_get_stdout(
    'git',
    ['-C', input_path, 'ls-files'],
    {
      show_cmd: false,
      throw_on_error: true
    }
  )
  ret.replace(/\n$/, '')
  if (ret === '') {
    return []
  } else {
    return ret.split('\n')
  }
}

/**
 * @return {String} full Git SHA of the source.
 */
function git_sha(input_path, src_branch) {
  const args = ['-C', input_path, 'log', '-n1', '--pretty=%H'];
  if (src_branch !== undefined) {
    args.push(src_branch);
  }
  return cmd_get_stdout('git', args, {show_cmd: false, throw_on_error: true}).slice(0, -1);
}

function git_toplevel(input_path) {
  return cmd_get_stdout('git', ['rev-parse', '--show-toplevel'], {
    show_cmd: false,
    throw_on_error: true
  }).slice(0, -1);
}

// https://stackoverflow.com/questions/37521893/determine-if-a-path-is-subdirectory-of-another-in-node-js
function is_subpath(parent, child) {
  const relative = path.relative(parent, child);
  return relative && !relative.startsWith('..') && !path.isAbsolute(relative);
}

function perf_print(name, ourbigbook_options) {
  if (ourbigbook_options === undefined || ourbigbook_options.log.perf) {
    console.error(`perf ${name} t=${performance.now()}`);
  }
}

/** Render a template file from under template/ */
function renderTemplate(templateRelpath, outdir, env) {
  const template = fs.readFileSync(
    path.join(ourbigbook_nodejs.PACKAGE_PATH, 'template', templateRelpath),
    ourbigbook_nodejs_webpack_safe.ENCODING
  );
  const out = (new Liquid()).parseAndRenderSync(
    template,
    env,
    {
      strictFilters: true,
      strictVariables: true,
    }
  );
  fs.writeFileSync(path.join(outdir, templateRelpath), out);
}

/** https://stackoverflow.com/questions/5827612/node-js-fs-readdir-recursive-directory-search
 *
 * @param {Set} skip_basenames
 * @param {Set} ignore_paths
 */
function* walk_files_recursively(file_or_dir, skip_basenames, ignore_paths) {
  if (fs.lstatSync(file_or_dir).isDirectory()) {
    const dirents = fs.readdirSync(file_or_dir, {withFileTypes: true});
    for (const dirent of dirents) {
      const res = path.join(file_or_dir, dirent.name);
      if (
        !skip_basenames.has(dirent.name) &&
        !ignore_paths.has(path.resolve(res))
      ) {
        if (dirent.isDirectory()) {
          yield* walk_files_recursively(res, skip_basenames, ignore_paths);
        } else {
          yield res;
        }
      }
    }
  } else {
    yield file_or_dir;
  }
}

// CLI options.
commander.option('--add-test-instrumentation', 'For testing only', false);
commander.option('--body-only', 'output only the content inside the HTLM body element', false);
commander.option('--china', 'https://docs.ourbigbook.com#china', false);
commander.option('--clear-db', 'clear the database before running', false);
commander.option('--dry-run', "don't run most external commands https://github.com/cirosantilli/ourbigbook#dry-run", false);
commander.option('--dry-run-push', "don't run git push commands https://github.com/cirosantilli/ourbigbook#dry-run-push", false);
commander.option('--embed-includes', 'https://docs.ourbigbook.com#embed-include', false);
commander.option('--embed-resources', 'https://docs.ourbigbook.com#embed-resources', false);
commander.option('--fakeroot <fakeroot>', 'Stop searching for ourbigbook.json at this directory rather than at the filesystem root');
commander.option('--generate <name>', 'https://docs.ourbigbook.com#generate', false);
commander.option('--help-macros', 'print the metadata of all macros to stdout in JSON format. https://docs.ourbigbook.com#help-macros', false);
commander.option('-l, --log <log...>', 'https://docs.ourbigbook.com#log');
commander.option('--no-html-x-extension', 'https://docs.ourbigbook.com#no-html-x-extension');
commander.option('--no-db', 'ignore the ID database, mostly for testing https://docs.ourbigbook.com#internal-cross-file-references-internals');
commander.option('--no-render', "only extract IDs, don't render: https://docs.ourbigbook.com#no-render");
commander.option('-T, --no-render-timestamp', "don't skip render by timestamp: https://docs.ourbigbook.com#no-render-timestamp");
commander.option('--outdir <outdir>', 'https://docs.ourbigbook.com#outdir');
commander.option('-o, --outfile <outfile>', 'https://docs.ourbigbook.com#outfile');
commander.option('-O, --output-format <output-format>', 'https://docs.ourbigbook.com#output-format', 'html');
commander.option('-p --publish', 'https://docs.ourbigbook.com#publish', false);
commander.option('-P, --publish-commit <commit-message>', 'https://docs.ourbigbook.com#publish-commit');
commander.option('--format-source', 'https://docs.ourbigbook.com#format-source');
commander.option('-S, --split-headers', 'https://docs.ourbigbook.com#split-headers', false);
commander.option('--stdout', 'also print output to stdout in addition to saving to a file https://docs.ourbigbook.com#stdout', false);
commander.option('--template <template>', 'https://docs.ourbigbook.com#template');
commander.option('-w, --watch', 'https://docs.ourbigbook.com#watch', false);
// Originally added for testing, this allows the test filesystems to be put under the repository iteslf,
// otherwise they would pickup our toplevel ourbigbook.json.
commander.option('-W, --web', 'sync to ourbigbook web https://docs.ourbigbook.com/#web', false);
commander.option('--web-id <id>', 'Upload only the selected ID. It must belong to a file being converted. https://docs.ourbigbook.com/#web-id', false);
commander.option('--web-url <url>', 'Set a custom sync URL for --web', false);
commander.option('--web-user <username>', 'Set username from CLI', false);
commander.option('--unsafe-ace', 'https://docs.ourbigbook.com#unsafe-ace', false);
commander.option('--unsafe-xss', 'https://docs.ourbigbook.com#unsafe-xss');
let inputPath;
commander.arguments(
  '[input_path]',
  undefined,
  'http://docs.ourbigbook.com#ourbigbook-executable',
).action(function (input_path) {
  inputPath = input_path;
});
commander.parse(process.argv);

// main action.
(async () => {
if (commander.helpMacros) {
  console.log(JSON.stringify(ourbigbook.macro_list(), null, 2));
} else if (commander.china) {
  console.log(china_dictatorship.get_data());
} else {
  let input;
  let title;
  let output;
  let publish = commander.publish || commander.publishCommit !== undefined;
  let html_x_extension;
  let input_dir;
  if (inputPath === undefined) {
    if (commander.web || publish || commander.watch || commander.generate) {
      inputPath = '.';
    }
  } else {
    if (commander.generate) {
      cli_error('canot give an input path with --generate');
    }
  }

  // Determine the ourbigbook.json file by walking up the directory tree.
  let input_path_is_file;
  if (inputPath === undefined) {
    // Input from stdin.
    input_dir = undefined;
    input_path_is_file = false;
  } else {
    if (!fs.existsSync(inputPath)) {
      cli_error('input path does not exist: ' + inputPath);
    }
    input_path_is_file = fs.lstatSync(inputPath).isFile();
    if (input_path_is_file) {
      input_dir = path.dirname(inputPath);
    } else {
      input_dir = inputPath;
    }
  }

  // Initialize ourbigbook.json and directories determined from it if present.
  let ourbigbook_json_dir;
  const ourbigbook_json = {};
  if (inputPath === undefined) {
    ourbigbook_json_dir = '.'
  } else {
    let curdir = path.resolve(inputPath);
    let initial_dir;
    if (fs.lstatSync(inputPath).isFile()) {
      curdir = path.dirname(curdir)
    }
    initial_dir = curdir;
    const fakeroot = commander.fakeroot === undefined ? undefined : path.resolve(commander.fakeroot)
    while (true) {
      const ourbigbook_json_path = path.join(curdir, ourbigbook.OURBIGBOOK_JSON_BASENAME);
      if (fs.existsSync(ourbigbook_json_path)) {
        Object.assign(ourbigbook_json, JSON.parse(fs.readFileSync(
          ourbigbook_json_path, ourbigbook_nodejs_webpack_safe.ENCODING)));
        ourbigbook_json_dir = curdir;
        break;
      }
      if (
        curdir === '/' ||
        curdir === fakeroot
      ) {
        break;
      }
      curdir = path.dirname(curdir)
    }
    if (ourbigbook_json_dir === undefined) {
      // No ourbigbook.json found.
      const cwd = process.cwd();
      if (is_subpath(cwd, inputPath)) {
        ourbigbook_json_dir = cwd
      } else {
        if (input_path_is_file) {
          ourbigbook_json_dir = path.dirname(inputPath)
        } else {
          ourbigbook_json_dir = inputPath
        }
      }
    }
  }
  let ignore;
  if (!('ignore' in ourbigbook_json)) {
    ourbigbook_json.ignore = [];
  }
  if (!('redirects' in ourbigbook_json)) {
    ourbigbook_json.redirects = [];
  }
  if (commander.web) {
    if (!('h' in ourbigbook_json)) {
      ourbigbook_json.h = {}
    }
    ourbigbook_json.h.splitDefault = true
  }

  let split_headers;
  if (publish) {
    // GitHub pages target is the only one for now.
    html_x_extension = false;
    split_headers = true;
  } else {
    html_x_extension = commander.htmlXExtension === false ? false : undefined;
    split_headers = commander.splitHeaders;
  }

  // Options that will be passed directly to ourbigbook.convert().
  if (!(commander.outputFormat in ourbigbook.OUTPUT_FORMATS)) {
    cli_error(`unknown output format: ${commander.outputFormat}`)
  }
  const output_format = (commander.formatSource || commander.web) ? ourbigbook.OUTPUT_FORMAT_OURBIGBOOK : commander.outputFormat
  const ourbigbook_options = {
    add_test_instrumentation: commander.addTestInstrumentation,
    body_only: commander.bodyOnly,
    ourbigbook_json,
    embed_includes: commander.embedIncludes,
    fs_exists_sync: (my_path) => fs.existsSync(path.join(ourbigbook_json_dir, my_path)),
    html_x_extension,
    output_format,
    outfile: commander.outfile,
    path_sep: path.sep,
    publish,
    read_include: read_include({
      exists: (inpath) => fs.existsSync(path.join(ourbigbook_json_dir, inpath)),
      read: (inpath) => fs.readFileSync(path.join(ourbigbook_json_dir, inpath), ourbigbook_nodejs_webpack_safe.ENCODING),
      path_sep: ourbigbook.Macro.HEADER_SCOPE_SEPARATOR,
    }),
    // Part of considering file previews for source code.
    //read_file: (readpath, context) => {
    //  readpath = path.join(path.dirname(context.options.input_path), readpath)
    //  if (
    //    fs.existsSync(readpath) &&
    //    // Let's prevent path transverasl a bit by default.
    //    path.resolve(readpath).startsWith(path.resolve(ourbigbook_json_dir))
    //  ) {
    //    return fs.readFileSync(readpath, ourbigbook_nodejs_webpack_safe.ENCODING);
    //  } else {
    //    return undefined
    //  }
    //},
    render: commander.render,
    split_headers: split_headers,
    template_vars: {},
    unsafe_xss: commander.unsafeXss,
  };

  ourbigbook_options.log = {};
  const non_ourbigbook_options_log = {};
  if (commander.log !== undefined) {
    for (const log of commander.log) {
      if (ourbigbook.LOG_OPTIONS.has(log)) {
        ourbigbook_options.log[log] = true;
      } else if (LOG_OPTIONS.has(log)) {
        non_ourbigbook_options_log[log] = true;
      } else {
        cli_error('unknown --log option: ' + log);
      }
    }
  }
  let template_path;

  if (commander.template !== undefined) {
    template_path = commander.template;
  } else if ('template' in ourbigbook_json) {
    template_path = path.join(ourbigbook_json_dir, ourbigbook_json.template);
  }
  if (template_path === undefined) {
    ourbigbook_options.template = undefined;
  } else {
    ourbigbook_options.template = fs.readFileSync(template_path).toString();
  }

  if (inputPath !== undefined) {
    try {
      ourbigbook_options.template_vars.git_sha = git_sha(input_dir);
    } catch(error) {
      // Not in a git repo.
    }
  }
  let outdir;
  if (commander.outdir === undefined) {
    if (commander.generate) {
      outdir = '.'
    } else {
      outdir = ourbigbook_json_dir;
    }
  } else {
    outdir = commander.outdir;
  }
  if (commander.generate) {
    let generate = commander.generate
    if (generate === 'subdir') {
      outdir = path.join(outdir, 'docs')
    }
    fs.mkdirSync(outdir, {recursive: true});

    // Generate package.json.
    const package_json = JSON.parse(fs.readFileSync(
      ourbigbook_nodejs.PACKAGE_PACKAGE_JSON_PATH).toString());
    const package_json_str = `{
  "dependencies": {
    "ourbigbook": "${package_json.version}"
  }
}
`;
    fs.writeFileSync(path.join(outdir, 'package.json'), package_json_str);

    // Generate .gitignore. Reuse our gitignore up to the first blank line.
    let gitignore_new = '';
    const gitignore = fs.readFileSync(
      ourbigbook_nodejs.GITIGNORE_PATH,
      ourbigbook_nodejs_webpack_safe.ENCODING
    );
    for (const line of gitignore.split('\n')) {
      if (line === '') {
        break;
      }
      gitignore_new += line + '\n';
    }
    fs.writeFileSync(path.join(outdir, '.gitignore'), gitignore_new);

    const new_ourbigbook_json = {};
    let title = 'Ourbigbook Template';
    let multifile
    if (generate === 'default') {
      renderTemplate('not-readme.bigb', outdir, {});
      multifile = true
    } else {
      title += ' ' + generate
      multifile = false
    }
    renderTemplate('README.bigb', outdir, {
      title,
      multifile,
      version: package_json.version,
    });
    if (multifile) {
      fs.copyFileSync(path.join(ourbigbook_nodejs.PACKAGE_PATH, 'main.liquid.html'),
        path.join(outdir, 'main.liquid.html'));
      fs.copyFileSync(path.join(ourbigbook_nodejs.PACKAGE_PATH, 'main.scss'),
        path.join(outdir, 'main.scss'));
      new_ourbigbook_json.template = 'main.liquid.html';
    }

    if (new_ourbigbook_json !== {}) {
      fs.writeFileSync(path.join(outdir, ourbigbook.OURBIGBOOK_JSON_BASENAME),
        JSON.stringify(new_ourbigbook_json, null, 2) + '\n');
    }
    process.exit(0);
  }
  let tmpdir
  const outputOutOfTree = ourbigbook_json.outputOutOfTree || commander.web
  if (
    // Possible on intput from stdin.
    outdir !== undefined
  ) {
    tmpdir = path.join(outdir, ourbigbook_nodejs_webpack_safe.TMP_DIRNAME);
    if (
      commander.outdir === undefined &&
      outputOutOfTree
    ) {
      outdir = path.join(tmpdir, output_format)
    }
  }
  // Options that are not directly passed to ourbigbook.convert
  // but rather used only by this ourbigbook executable.
  const non_ourbigbook_options = {
    ourbigbook_json_dir,
    ourbigbook_paths_converted: [],
    ourbigbook_paths_converted_only: false,
    commander,
    db_options: {},
    file_rows_dict: {},
    encoding: ourbigbook_nodejs_webpack_safe.ENCODING,
    external_css_and_js: false,
    had_error: false,
    is_render_after_extract: false,
    log: non_ourbigbook_options_log,
    out_css_path: ourbigbook_nodejs.DIST_CSS_PATH,
    out_js_path: ourbigbook_nodejs.DIST_JS_PATH,
    outdir,
    post_convert_callback: undefined,
    publish,
  };
  ourbigbook_options.outdir = path.relative(outdir, ourbigbook_json_dir)
  if (!non_ourbigbook_options_log.db) {
    // They do not like true, has to be false or function.
    // And setting undefined is also considered true.
    non_ourbigbook_options.db_options.logging = false;
  }
  let input_git_toplevel;
  let subdir_relpath;
  let publish_tmpdir;

  if (inputPath === undefined) {
    // Input from stdin.
    title = 'stdin';
    input = fs.readFileSync(0, ourbigbook_nodejs_webpack_safe.ENCODING);
    output = await convert_input(input, ourbigbook_options, non_ourbigbook_options);
  } else {
    if (!fs.existsSync(inputPath)) {
      cli_error(`input_path does not exist: "${inputPath}"`);
    }
    let publish_dir;
    let cmd_options = {
      dry_run: commander.dryRun,
      throw_on_error: true,
    }
    if (!input_path_is_file) {
      if (commander.outfile !== undefined) {
        cli_error(`--outfile given but multiple output files must be generated, maybe you want --outdir?`);
      }
      if (publish) {
        input_git_toplevel = git_toplevel(inputPath);
        subdir_relpath = path.relative(input_git_toplevel, inputPath);
        publish_dir = path.join(tmpdir, 'publish');
        publish_git_dir = path.join(publish_dir, '.git');
        if (fs.existsSync(publish_git_dir)) {
          // This cleanup has to be done before the database initialization.
          cmd_get_stdout('git', ['-C', publish_dir, 'clean', '-x', '-d', '-f'], cmd_options);
        }
        publish_tmpdir = path.join(publish_dir, subdir_relpath, ourbigbook_nodejs_webpack_safe.TMP_DIRNAME);
      }
    }
    if (publish_tmpdir === undefined) {
      publish_tmpdir = tmpdir;
    }

    // OURBIGBOOK_TEX_BASENAME
    let tex_path = path.join(ourbigbook_json_dir, OURBIGBOOK_TEX_BASENAME);
    ourbigbook_options.katex_macros = ourbigbook_nodejs_webpack_safe.preload_katex(tex_path)

    // Setup the ID database.
    if (commander.db) {
      non_ourbigbook_options.db_options.storage = path.join(publish_tmpdir, 'db.sqlite3');
    } else {
      non_ourbigbook_options.db_options.storage = SQLITE_MAGIC_MEMORY_NAME
    }
    if (commander.web) {
      let token
      const web_url = commander.webUrl ? commander.webUrl : ourbigbook.WEB_URL
      const url = new URL(web_url)
      const host = url.host
      await create_db(ourbigbook_options, non_ourbigbook_options);
      const host_row = await non_ourbigbook_options.sequelize.models.Cli.findOne({ where: { host } })
      const webApi = new WebApi({
        getToken: () => token,
        https: url.protocol === 'https:',
        port: url.port,
        hostname: url.hostname,
        validateStatus: () => true,
      })
      function handleWebApiErr(err) {
        if (err.code === 'ECONNREFUSED') {
          cli_error('could not connect to server');
        } else {
          throw err
        }
      }
      let username
      if (host_row) {
        token = host_row.token
        username = host_row.username
      } else {
        let err, password
        if (commander.webUser) {
          username = commander.webUser
        } else {
          ;[err, username] = await read({ prompt: 'Username: ' })
        }
        ;[err, password] = await read({ prompt: 'Password: ', silent: true })
        let data, status
        try {
          ;({ data, status } = await webApi.userLogin({ username, password }))
        } catch(err) {
          handleWebApiErr(err)
        }
        if (status === 422) {
          cli_error('invalid username or password');
        } else if (status !== 200) {
          cli_error(`error status: ${status}`);
        }
        token = data.user.token
        await non_ourbigbook_options.sequelize.models.Cli.create({ host, token, username })
      }
      ourbigbook_options.split_headers = true
      ourbigbook_options.render_include = false
      const titleRegex = new RegExp(`${ourbigbook.INSANE_HEADER_CHAR} (.*)`)
      // We create this quick and dirty separate database to store information for upload.
      // Technically much of this information is part of Article, but factoring that would be risky/hard,
      // it is not worth it.
      //
      // Adding this cache because I had an unminimizable error on the main document, and we have to save some time
      // or else I can't minimize it, this way we can skip the initial bigb split render conversion and go
      // straight to upload.
      const sequelizeWeb = new Sequelize({
        dialect: 'sqlite',
        storage: path.join(non_ourbigbook_options.outdir, 'web.sqlite'),
        logging: false,
      })
      const sequelizeWebArticle = sequelizeWeb.define('Article', {
        idid: { type: DataTypes.TEXT, unique: true },
        title: { type: DataTypes.TEXT },
        body: { type: DataTypes.TEXT },
        inpath: { type: DataTypes.TEXT },
        parentId: { type: DataTypes.TEXT },
      });
      const sequelizeWebIndexId = sequelizeWeb.define('IndexId', {
        idid: { type: DataTypes.TEXT },
        uniqueHack: { type: DataTypes.INTEGER, unique: true },
      });
      await sequelizeWeb.sync()
      non_ourbigbook_options.post_convert_callback = async (inpath, extra_returns) => {
        if (extra_returns.errors.length === 0) {
          const rendered_outputs = extra_returns.rendered_outputs
          for (let inpath in rendered_outputs) {
            const rendered_outputs_entry = rendered_outputs[inpath]
            if (rendered_outputs_entry.split) {
              // To convert:
              //
              // linux-kernel-module-cheat-split.bigb
              //
              // to:
              //
              // linux-kernel-module-cheat.bigb
              //
              // on:
              //
              // = Linux kernel module cheat
              // {splitSuffix}
              //
              // otherwise the ID becomes linux-kernel-module-cheat and \x links fail.
              const source = rendered_outputs_entry.full;
              const lines = source.split('\n')
              let title = lines[0].match(titleRegex)[1]
              const inpathParse = path.parse(inpath)
              const pathNoext = path.join(inpathParse.dir, inpathParse.name)
              if (rendered_outputs_entry.split_suffix) {
                inpath = pathNoext.slice(0, -(rendered_outputs_entry.split_suffix.length + 1)) + `.${ourbigbook.OURBIGBOOK_EXT}`
              }
              let addId
              let addSubdir
              let isToplevelIndex = false
              const header_ast = rendered_outputs_entry.header_ast
              if (ourbigbook.INDEX_FILE_BASENAMES_NOEXT.has(inpathParse.name)) {
                if (inpathParse.dir) {
                  const dirPathParse = path.parse(inpathParse.dir)
                  const titleId = ourbigbook.title_to_id(title)
                  if (titleId !== dirPathParse.name) {
                    // This would be ideal, allowing us to store all information about the article in the body itself.
                    // But it was hard to implement, since now the input path is an important input of conversion.
                    // So to start with we will just always provide the input path as a separate parameter.
                    // {id= for toplevel was ignored as of writing, which is bad, should be either used or error.
                    //addId = dirPathParse.name
                  }
                  if (dirPathParse.dir) {
                    // Same as addId
                    //addSubdir = dirPathParse.dir
                  }
                } else {
                  title = 'Index'
                  await sequelizeWebIndexId.upsert({ idid: header_ast.id, uniqueHack: 0 })
                  isToplevelIndex = true
                }
                inpath = `index.${ourbigbook.OURBIGBOOK_EXT}`
              } else {
                const titleId = ourbigbook.title_to_id(title)
                if (titleId !== inpathParse.name) {
                  //addId = inpathParse.name
                }
                if (inpathParse.dir) {
                  //addSubdir = inpathParse.dir
                }
              }
              let bodyStart
              if (lines[1] === '' && !addId && !addSubdir) {
                bodyStart = 2
              } else {
                bodyStart = 1
              }
              let body = ''
              if (addId) {
                // Restore this if we ever remove the separate path magic input.
                // Also id of toplevel header is currently ignored as of writing:
                //body += `{id=${addId}}\n`
              }
              if (addSubdir) {
                // Restore this if we ever remove the separate path magic input.
                //body += `{subdir=${addSubdir}}\n`
              }
              body += lines.slice(bodyStart).join('\n')
              const parent_ast = rendered_outputs_entry.header_ast.get_header_parent_asts(extra_returns.context)[0]
              const article = { title, body, inpath }
              if (parent_ast) {
                let parentId = `${ourbigbook.AT_MENTION_CHAR}${username}`
                if (
                  parent_ast.id !== ourbigbook.INDEX_BASENAME_NOEXT &&
                  // Force every child of the topevel to add it as "@username" and instead of deducing it from title
                  // as done on CLI. This means that giving the toplevel a custom ID and using that ID will fail to upload...
                  // there is no solution to that. We should just force the toplevel to have no ID then on CLI for compatibility?
                  !(
                    parent_ast.is_first_header_in_input_file &&
                    ourbigbook.INDEX_FILE_BASENAMES_NOEXT.has(path.parse(parent_ast.source_location.path).name)
                  )
                ) {
                  parentId = parentId + `/${parent_ast.id}`
                }
                article.parentId = parentId
              }
              let id_to_article_key
              if (isToplevelIndex) {
                id_to_article_key = ''
              } else {
                id_to_article_key = header_ast.id
              }
              article.idid = id_to_article_key
              await sequelizeWebArticle.upsert(article)
            }
          }
        }
      }
      let treeToplevelId
      let treeDefinedAt
      if (input_path_is_file) {
        await convert_path_to_file(inputPath, ourbigbook_options, non_ourbigbook_options);
        treeToplevelId = (await non_ourbigbook_options.sequelize.models.File.findOne({ where: { path: inputPath } })).toplevel_id
        treeDefinedAt = inputPath
      } else {
        // TODO non toplevel directory not supported yet.
        await convert_directory_extract_ids_and_render(
          inputPath,
          ourbigbook_options,
          non_ourbigbook_options,
        );
        treeToplevelId = (await sequelizeWebIndexId.findAll())[0].idid
      }
      let header_tree = []
      if (input_path_is_file || commander.webId) {
        let toPush
        if (commander.webId) {
          toPush = commander.webId
        } else {
          toPush = treeToplevelId
        }
        header_tree.push({ to_id: toPush })
      } else {
        // Fake an index entry at the end so that the index will get rendered.
        // It is not otherwise present as it has no parents.
        header_tree.push({ to_id: '' })
      }
      if (!commander.webId) {
        header_tree = header_tree.concat(await non_ourbigbook_options.db_provider.fetch_header_tree_ids(
          [treeToplevelId],
          {
            // By following reverse order we don't have to worry about setting up previousSiblingId:
            // we just use the fact that the API automatically inserts as first sibling when not given.
            // TODO actually add siblings later on, as that will allow updating the page tree.
            to_id_index_order: 'DESC',
            definedAt: treeDefinedAt,
          }
        ))
      }
      console.error('\nUpload');
      const dorender = [false]
      if (commander.render) {
        dorender.push(true)
      }
      for (const render of dorender) {
        // This ordering ensures parents come before children.
        for (const header_tree_entry of header_tree) {
          const id = header_tree_entry.to_id
          const article = await sequelizeWebArticle.findOne({ where: { idid: id } })
          if (
            // Can fail for synonyms.
            article !== null
          ) {
            let pref
            if (render) {
              pref = MESSAGE_PREFIX_RENDER
            } else {
              pref = MESSAGE_PREFIX_EXTRACT_IDS
            }
            console.error(`${pref}: ${article.title} (${article.inpath})`);
            let data, status
            const inpathParse = path.parse(article.inpath)
            try {
              ;({ data, status } = await webApi.articleCreateOrUpdate(
                { titleSource: article.title, bodySource: article.body },
                {
                  // TODO https://docs.ourbigbook.com/todo/remove-the-path-parameter-from-the-article-creation-api
                  path: path.join(inpathParse.dir, inpathParse.name),
                  render,
                  parentId: article.parentId === null ? undefined : article.parentId,
                }
              ))
            } catch(err) {
              handleWebApiErr(err)
            }
            //console.log(require('child_process').execSync(`printf 'count '; sqlite3 /home/ciro/bak/git/ourbigbook/web/db.sqlite3 "select to_id_index,from_id,to_id,defined_at from Ref where from_id = '@barack-obama' and type = 0 order by to_id_index" | wc -l`).toString())
            //console.log(require('child_process').execSync(`sqlite3 /home/ciro/bak/git/ourbigbook/web/db.sqlite3 "select to_id_index,from_id,to_id,defined_at from Ref where from_id = '@barack-obama' and type = 0 order by to_id_index"`).toString())
            if (status !== 200) {
              console.error(`error status: ${status}`);
              const errors = data.errors
              if (errors instanceof Array) {
                for (const error of data.errors) {
                  console.error(error);
                }
              } else {
                if (errors === undefined) {
                  console.error(data);
                } else {
                  console.error(errors);
                }
              }
              cli_error('conversion error')
            }
          }
        }
      }
    } else if (commander.watch) {
      if (commander.stdout) {
        cli_error('--stdout and --watch are incompatible');
      }
      if (publish) {
        cli_error('--publish and --watch are incompatible');
      }
      await create_db(ourbigbook_options, non_ourbigbook_options);
      if (!input_path_is_file) {
        await reconcile_db_and_filesystem(inputPath, ourbigbook_options, non_ourbigbook_options);
        await convert_directory_extract_ids(inputPath, ourbigbook_options, non_ourbigbook_options);
      }
      const watcher = require('chokidar').watch(inputPath, {ignored: DEFAULT_IGNORE_BASENAMES})
      const convert = async (subpath) => {
        await convert_path_to_file(subpath, ourbigbook_options, non_ourbigbook_options);
        await check_db(non_ourbigbook_options)
        non_ourbigbook_options.ourbigbook_paths_converted = []
      }
      watcher.on('change', convert).on('add', convert)
    } else {
      if (input_path_is_file) {
        if (publish) {
          cli_error('--publish must take a directory as input, not a file');
        }
        await create_db(ourbigbook_options, non_ourbigbook_options);
        output = await convert_path_to_file(inputPath, ourbigbook_options, non_ourbigbook_options);
        await check_db(non_ourbigbook_options)
      } else {
        if (commander.stdout) {
          cli_error('--publish cannot be used in directory conversion');
        }
        let actual_input_dir;
        let publish_branch;
        let publish_out_publish_dir;
        let publish_out_publish_dist_dir;
        let publish_remote_url;
        let src_branch;

        if (publish) {
          // Clone the source to ensure that only git tracked changes get built and published.
          if (!git_is_in_repo(inputPath)) {
            cli_error('--publish must point to a path inside a git repository');
          }
          const origin_url = cmd_get_stdout('git', ['-C', inputPath, 'config', '--get', 'remote.origin.url'], cmd_options).slice(0, -1);
          if (ourbigbook_json.publishRemoteUrl) {
            publish_remote_url = ourbigbook_json.publishRemoteUrl
          } else {
            publish_remote_url = origin_url
          }
          if (!publish_remote_url) {
            publish_remote_url = 'git@github.com:cirosantilli/ourbigbook.git';
          }
          src_branch = cmd_get_stdout('git', ['-C', inputPath, 'rev-parse', '--abbrev-ref', 'HEAD'], cmd_options).slice(0, -1);
          if (commander.dryRun) {
            src_branch = 'master';
          }
          const parsed_remote_url = require("git-url-parse")(publish_remote_url);
          if (parsed_remote_url.source !== 'github.com') {
            cli_error('only know how  to publish to origin == github.com currently, please send a patch');
          }
          let remote_url_path_components = parsed_remote_url.pathname.split(path.sep);
          if (remote_url_path_components[2].startsWith(remote_url_path_components[1] + '.github.io')) {
            publish_branch = 'master';
          } else {
            publish_branch = 'gh-pages';
          }
          if (
            publish_remote_url === origin_url &&
            src_branch === publish_branch
          ) {
            cli_error(`source and publish branches are the same: ${publish_branch}`);
          }
          fs.mkdirSync(publish_dir, {recursive: true});
          if (commander.publishCommit !== undefined) {
            cmd_get_stdout('git', ['-C', inputPath, 'add', '-u'], cmd_options);
            cmd_get_stdout( 'git', [ '-C', inputPath, 'commit', '-m', commander.publishCommit ], cmd_options);
          }
          if (fs.existsSync(publish_git_dir)) {
            cmd_get_stdout('git', ['-C', publish_dir, 'checkout', '--', '.'], cmd_options);
            cmd_get_stdout('git', ['-C', publish_dir, 'pull'], cmd_options);
            cmd_get_stdout('git', ['-C', publish_dir, 'submodule', 'update', '--init'], cmd_options);
          } else {
            cmd_get_stdout('git', ['clone', '--recursive', input_git_toplevel, publish_dir],
              ourbigbook.clone_and_set(cmd_options, 'dry_run', false));
          }

          // Set some variables especially for publishing.
          actual_input_dir = path.join(publish_dir, subdir_relpath);
          non_ourbigbook_options.ourbigbook_json_dir = actual_input_dir;
          publish_out_publish_dir = path.join(publish_tmpdir, 'publish');
          publish_out_publish_dist_dir = path.join(publish_out_publish_dir, ourbigbook_nodejs.DIST_BASENAME)
          non_ourbigbook_options.out_css_path = path.join(publish_out_publish_dist_dir, ourbigbook_nodejs.DIST_CSS_BASENAME);
          non_ourbigbook_options.out_js_path = path.join(publish_out_publish_dist_dir, ourbigbook_nodejs.DIST_JS_BASENAME);
          non_ourbigbook_options.external_css_and_js = true;
          // Remove all files from the gh-pages repository in case some were removed from the original source.
          if (
            fs.existsSync(path.join(publish_out_publish_dir, '.git'))
          ) {
            if (git_ls_files(publish_out_publish_dir).length > 0) {
              cmd_get_stdout('git', ['-C', publish_out_publish_dir, 'rm', '-r', '-f', '.'], cmd_options);
            }
          } else {
            fs.mkdirSync(publish_out_publish_dir, {recursive: true});
          }
        } else {
          actual_input_dir = inputPath;
          publish_out_publish_dir = outdir;
        }
        non_ourbigbook_options.outdir = publish_out_publish_dir;
        await create_db(ourbigbook_options, non_ourbigbook_options);

        // Do the actual conversion.
        await convert_directory_extract_ids_and_render(actual_input_dir, ourbigbook_options, non_ourbigbook_options)

        // Generate redirects from ourbigbook.json.
        for (let [from, to] of ourbigbook_json.redirects) {
          if (
            // TODO https://docs.ourbigbook.com/respect-ourbigbook-json-htmlxextension-on-ourbigbook-json-redirects
            ourbigbook_options.html_x_extension === false ? false : true &&
            !ourbigbook.protocol_is_known(to)
          ) {
            to +=  '.' + ourbigbook.HTML_EXT
          }
          generate_redirect_base(
            path.join(non_ourbigbook_options.outdir, from + '.' + ourbigbook.HTML_EXT),
            to
          )
        }

        // Publish the converted output if build succeeded.
        if (publish && !non_ourbigbook_options.had_error) {
          // Push the original source.
          if (!commander.dryRunPush) {
            cmd_get_stdout('git', ['-C', inputPath, 'push'], cmd_options);
          }
          cmd_get_stdout('git', ['-C', publish_out_publish_dir, 'init'], cmd_options);
          // https://stackoverflow.com/questions/42871542/how-to-create-a-git-repository-with-the-default-branch-name-other-than-master
          cmd_get_stdout('git', ['-C', publish_out_publish_dir, 'checkout', '-B', publish_branch], cmd_options);
          try {
            // Fails if remote already exists.
            cmd_get_stdout('git', ['-C', publish_out_publish_dir, 'remote', 'add', 'origin', publish_remote_url], cmd_options);
          } catch(error) {
            cmd_get_stdout('git', ['-C', publish_out_publish_dir, 'remote', 'set-url', 'origin', publish_remote_url], cmd_options);
          }
          // Ensure that we are up-to-date with the upstream gh-pages if one exists.
          cmd_get_stdout('git', ['-C', publish_out_publish_dir, 'fetch', 'origin'], cmd_options);
          cmd_get_stdout(
            'git',
            ['-C', publish_out_publish_dir, 'reset', `origin/${publish_branch}`],
            // Fails on the first commit in an empty repository.
            ourbigbook.clone_and_set(cmd_options, 'throw_on_error', false)
          );

          // Generate special files needed for GitHub pages.
          gemfile_content = "gem 'github-pages', group: :jekyll_plugins\n";
          fs.writeFileSync(path.join(publish_out_publish_dir, 'Gemfile'), gemfile_content);

          // Commit and push.
          if ('prepublish' in ourbigbook_json) {
            if (!commander.dryRun && !commander.dryRunPush && !commander.unsafeAce) {
              cli_error('prepublish in ourbigbook.json requires running with --unsafe-ace');
            }
            const prepublish_path = ourbigbook_json.prepublish
            if (!fs.existsSync(prepublish_path)) {
              cli_error(`${ourbigbook.OURBIGBOOK_JSON_BASENAME} prepublish file not found: ${prepublish_path}`);
            }
            try {
              cmd_get_stdout(path.resolve(prepublish_path), [publish_out_publish_dir]);
            } catch(error) {
              cli_error(`${ourbigbook.OURBIGBOOK_JSON_BASENAME} prepublish command exited non-zero, aborting`);
            }
          }

          // Copy runtime assets from dist/ into the output repository.
          const dir = fs.opendirSync(ourbigbook_nodejs.DIST_PATH)
          let dirent
          while ((dirent = dir.readSync()) !== null) {
            require('fs-extra').copySync(
              path.join(ourbigbook_nodejs.DIST_PATH, dirent.name),
              path.join(publish_out_publish_dist_dir, dirent.name)
            )
          }
          dir.closeSync()
          cmd_get_stdout('git', ['-C', publish_out_publish_dir, 'add', '.'], cmd_options);
          source_commit = git_sha(inputPath, src_branch);
          const args = ['-C', publish_out_publish_dir, 'commit', '-m', source_commit]
          if (git_has_commit(publish_out_publish_dir)) {
            args.push('--amend')
          }
          let commit_cmd_options = cmd_options
          if (ourbigbook_json.publishCommitDate) {
            args.push(...['--date', ourbigbook_json.publishCommitDate])
            commit_cmd_options = Object.assign({}, cmd_options)
            commit_cmd_options.env_extra = { GIT_COMMITTER_DATE: ourbigbook_json.publishCommitDate }
          }
          cmd_get_stdout('git', args, commit_cmd_options);
          if (!commander.dryRunPush) {
            cmd_get_stdout('git', ['-C', publish_out_publish_dir, 'push', '-f', 'origin', `${publish_branch}:${publish_branch}`], cmd_options);
            // Mark the commit with the `published` branch to make it easier to find what was last published.
            cmd_get_stdout('git', ['-C', inputPath, 'checkout', '-B', 'published'], cmd_options);
            cmd_get_stdout('git', ['-C', inputPath, 'push', '-f', '--follow-tags'], cmd_options);
            cmd_get_stdout('git', ['-C', inputPath, 'checkout', '-'], cmd_options);
          }
        }
      }
    }
  }
  if (
    inputPath === undefined ||
    (
      output !== undefined &&
      commander.stdout
    )
  ) {
    process.stdout.write(output);
  }
  perf_print('exit', ourbigbook_options)
  if (!commander.watch) {
    process.exit(non_ourbigbook_options.had_error);
  }
}
})().catch((e) => {
  console.error(e);
  process.exit(1);
})
